# DIPLOM_2024

# **Данная работа посвящена изучению рынка продаж подержанных автомобилей

# **Актуальной остается задача:

Изучение рынка продаж подержанных автомобилей       

Актуальной остается задача: 
Предсказание цены на подержанные автомобиль (на примере марки) Ford

# **Бизнес-постановка задачи

Оценка подержанного автомобиля - это достаточно трудная задача, 

так как на стоимость влияют различные факторы, например:

 возраст автомобиля, его состояние, пробег и даже личное отношение продавца. 
 
 Таким образом, цена подержанных автомобилей на рынке не является постоянной. И поскольку нет прозрачности в её образовании, а спрос растет ежегодно, у нечестных предпринимателей возникает стимул иррационально завышать цену. 
 
# **Предложенный проект - это

 Модель для оценки стоимости подержанного автомобиля, которая помогла бы 
 
 1) покупателям не переплатить за желаемое авто, а 
 
 2) честным продавцам быстро устанавливать цену, соответствующую их предложениям.

Постановка задачи анализа данных

Целью данной задачи является прогнозирование цены на подержанные автомобили Ford с помощью построения регрессионных моделей и их анализа. 

Отмечу, что Набор данных состоит из информации о транспортных средствах, выставленных на продажу на сайте Craigslist. 

Данные опубликованы в открытом доступе на платформе Kaggle.

# **Платформа Kaggle 

Специалистам в области Data Science необходимо постоянно учиться и улучшать свои навыки.

 Платформа Kaggle помогает начинающим дата-сайентистам практиковаться на реальных данных, а опытным — изучать работу коллег и соревноваться с ними.

 Kaggle — это сообщество специалистов по Data Science. Здесь можно изучать машинное обучение, писать свои и разбирать чужие прогнозные модели, участвовать в соревнованиях и общаться с дата-сайентистами. Сервис полностью бесплатен.


Kaggle используют и начинающие, и опытные дата-сайентисты со всего мира.

 Есть пользовательский рейтинг — очки в нем можно заработать за решение задач по машинному обучению, обсуждение на форуме, публикацию своего кода и наборов данных.
 
 # **ВАЖНО!!! Многие компании при найме обращают внимание на место соискателя в рейтинге Kaggle.

Поможет освоить основные принципы Machine Learning и Data Science

В разделе Learn есть больше десятка полезных курсов: введение в SQL, введение в машинное обучение, Python, визуализация данных и другие. Курсы не объяснят математику, стоящую за алгоритмами машинного обучения, но научат принципам, необходимым для анализа данных. Это поможет сэкономить время, которое обычно тратится на пассивное изучение материала.

Быстро погрузит в практику

Вместо того чтобы искать задачи по изученной теории, можно начать работать над проектом и уже в процессе «добирать» необходимые знания. Так обучение Machine Learning и Data Science проходит увлекательнее и приносит больше пользы.

Поможет решать актуальные проблемы на реальных данных

Kaggle публикует соревнования, которые инициируют компании — они ищут решения актуальных проблем и дают участникам реальные наборы данных. Это дает возможность не только получить опыт в решении задач, но и начать взаимодействовать с компаниями и их запросами.

# **Шаг 1: получите базовые знания

Выберите язык программирования — например, Python или R — и изучить его основы. Затем перейти к Kaggle Learn, чтобы закрепить знания по выбранному языку программирования, начать погружение в машинное обучение и познакомиться с методами визуализации данных.

# **Шаг 2: найти интересный проект или набор данных

Для начала можно выбрать несложный конкурс и испытать себя. На этом этапе начинающим дата-сайентистам помогут Kernels («ядра») — онлайн-среда для программирования, которая работает на серверах Kaggle. В ней можно писать Python/R-скрипты и работать в Jupyter Notebooks.

Kernels бесплатны и отлично подходят для тестирования. Можно скопировать или изменить уже существующее «ядро» другого пользователя, а также поделиться своим с сообществом.

# **Шаг 3: изучить открытые «ядра»

Анализ открытых «ядер» поможет сравнить свой код с кодом других пользователей и понять, какие разделы Machine Learning и Data Science следует изучить тщательнее. Это ускорит погружение в тему и сделает процесс более осознанным.

# **Шаг 4: опубликовать собственное «ядро»

Создаqnt собственный Kernel — даже если у вас еще нет уверенности в своих силах. «Ядро» лучше сделать публичным: так можно заработать больше очков на платформе и получить обратную связь.

# **Шаг 5: изучите новую информацию и снова опубликуйте «ядро»

Следуйте принципу «Learn, leap and repeat» (научись, сделай большой шаг вперед и начни снова): усовершенствуйте свои навыки и опять вернитесь к шагу 4.

# **Шаг 6: совершенствуйте анализ, изучая Kernels

На этой стадии у начинающего дата-сайентиста обычно уже есть свои методы работы с данными и прогнозирующие модели — поэтому еще раз изучите «ядра» других пользователей. Можно задать коллегам вопрос, начать дискуссию или просто дополнить свои наработки.

Опытный дата-сайентист

Если у вас уже есть опыт, то вы сможете участвовать в соревнованиях по исследованию данных — в одиночку или командой решать задачи по машинному обучению. 

Однако опытные специалисты соревнуются не только из интереса: призеры соревнований получают денежные призы, становятся известными в сообществе, их приглашают на престижные позиции.

Например, в конце 2020 года стартовал конкурс «Взлом почки».

Смысл  Задачи  для специалистов — находить ткани определенного типа на изображениях.

 Это часть проекта Human BioMolecular Atlas Program (HuBMAP) по изучению работы человеческого организма на клеточном уровне.
 
 Стимулом и бонусом для участников: Призовой фонд — $60 000.

# **МИССИЯ   DATA  SCIENTISTа  -  решать по истине  амбициозные  задачи!

Они  учатся 
- создавать искусственный  интеллект
- обучать нейронные сети
- менять мир к лучшему
- и конечно же отлично зарабатывать, занимаясь любимым делом!

# **ВКЛАД  GeekBrains в направлении подготовки студентов по данному направлению неоспоримый!

GeekBrains  не имеет себе  равных

Специалисты, наставники, преподаватели, кураторы -  профессионалы с большой буквы в своем деле!


# **ТЕПЕРЬ   К  ДЕЛУ!

Обзор доступных данных

В выборке 4913 наблюдений и 12 характеристик для каждого из объектов (штат продажи, год выпуска, технические характеристики автомобиля, цена транспортного средства и т.д.).

 Пустые значения указывают на то, что о соответствующей характеристики нет информации.
 
  Выборку разбиваю на две части:
  
  1-я  для обучения и 
  
  2-я для тестирования модели.

Итак, данные содержат два типа переменных:

Целевая: price

Остальные переменные: 11 переменных, которые могут использоваться для прогноза целевой переменной.

# **План-алгоритм анализа данных (data mining):

1. Загрузить данные для обучения

2. Обработать данные перед обучением модели

3. Обучить модель на обучающей выборке

4. Загрузить и предобработать данные для тестирования

5. Провалидировать модель на тестовой выборке

Р Е А Л И З А Ц И Я                П Л А Н А

# **1. Загрузить данные для обучения



**Шаг 1.1. Загружаем библиотеки**

Библиотека **warnings** отвечает за то, какие предупреждения (warnings) о работе будут выводиться пользователю.

FutureWarning - предупреждения о том, как изменится работа библиотек в будущих версиях.

Поэтому такие предупреждения следует игнорировать.

Чтобы включить режим игнорирования отбираем все предупреждения из категории FutureWarning и выбираем для них действия 'ignore'.

Данное действие  делается вызовом функции simplefilter c задание двух атрибутов:

 - действия action и 
 
 - категории предупреждений category.


   Для корректной работы с данными в python требуется загрузить специальную библиотеку pandas, программную библиотеку на языке python для обработки и анализа данных.

   Для корректной работы с графиками в python требуется загрузить специальную библиотеку matplotlib, программную библиотеку на языке python для визуализации данных двумерной и трехмерной графикой.

Графики используются для облегчения интерпретации полученных результатов, а также в качестве иллюстраций в презентациях и отчетах.

Оснвные методы для построения:

plot() - графики
semilogy() - график логарифметический
hist() - гистограммы

**Шаг 1.2. Загрузим данные**



Для решения задачи мы будем использовать данные. 

Они состоят из двух частей: 

- часть для обучения и 

- часть для тестирования модели. 

Загружаем данные с помощие команды !wget.

 Для того, чтобы игнорировать сообщения в процессе загрузки используем магическую команду %%capture в первой строке.
 

 Исходные данные скачались

Наблюдаем их в загруженных файлах

Так как данные в формате xlsx (Excel), мы будем использовать специальную функцию из библиотеки pandas для загрузки таких данных read_excel.

В функции передаем один атрибут: название таблицы с данными.

Команда *head * выводит содержимое данных

# **Шаг 1.3.
Посмотрим на размеры загруженной таблицы, у которой мы видели только первые 5 строк.

Для этого вызываем поле shape у нашей переменной training_data.

Поле вызывается также как метод, но в конце скобки не ставятся, так как для поля не предусмотрена передача аргументов.

Первое и второе числа в скобках означают следующее:

Итак, таблица содержит 4913 строк (объектов) и 12 столбцов (признаков), включая выходной (целевой) признак.

Таблицу проверили, теперь можно приступать к обработке данных.

Последнее действие очень важное и  означает до 70 - 90 процентов всей работы специалиста при обучении ЛЮБОЙ МОДЕЛИ!

## 2. Обработать данные перед обучением модели


# **Шаг 2.1. Проверяем данные на наличие пропусков и типов переменных**

Начнем с проверки общей информации о данных. Для того чтобы это сделать, нужно обратиться вызвать у переменной training_data метод info().

Напомним, что в конце необходимо поставить скобочки.

# **Шаг 2.2. Удаляем пропуски

Как мы уже видели выше, в наших данных есть пропуски (значения NaN). Для удобства работы выкинем такие данные из нашего датасета, применив метод dropna() к training_data

# **Гистограмма - 
это способ графического представления табличных данных, благодаря которому можно увидеть распределение значений признака.

Для построения гистограммы необходимо вызвать метод hist() у объекта training_data. Желательно указать аргумент figsize, который устанавливает ожидаемый размер изображения. В нашем случае это (15,15).

# **Шаг 2.3 Кодируем категориальные признаки

Категориальный признак - это такой признак, который может принимать одно значение из ограниченного числа возможных.

В наших данных есть два числовых категориальных признаков: condition, cylinders

И несколько текстовых категориальных признаков: title_status, transmission, drive, size.

Машине сложно обрабатывать текстовые признаки, поэтому нам необходимо закодировать их, то есть преобразовать в числовые. Пример кодирования для категориального признака Category, принимающего одно из четырех возможных значений ['Human', 'Penguin', 'Octopus', 'Alien'].

image.png

Предположим, что некоторый признак может принимать n разных значений. Применив One Hot Encoding, мы создадим n признаков, все из которых для каждой строчки равны нулю за исключением одного. На позицию, соответствующую значению категории признака, мы помещаем 1.

Рассмотрим на уже знакомом примере. Пусть имеется категориальный признак Category, принимающий одно из четырех возможных значений ['Human', 'Penguin', 'Octopus', 'Alien']. После применения One Hot Encoding мы получим четыре новых признака (по количеству возможных значений) is_Human, is_Penguin, is_Octopus, is_Alien. Для той строчки, у которой в исходных данных стояла категория Human, в столбце is_Human будет стоять 1, в остальных столбцах 0. Аналогично для другого значения.

Посмотрим, как изменится качество модели с кодированием признаков с помощью One Hot Encoding
Заметим, что название переменной, по которой строится гистограмма, указано в названии графика.

**Шаг 2.4. Работаем с целевой переменной**

*Какая переменная целевая?*

В данном случае по условию задачи мы должны прогнозировать стоимость автомобиля, поэтому целевая переменная - это price.

Нам нужно выделить в отдельную переменную *training_values* столбец из нашей таблицы, который соответствует определенной выше целевой переменной.


Отделим входные переменные от выходной (целевой), чтобы можно было построить модель предсказания целевой переменной по входным.

Для это нужно у переменной training_data вызвать метод

drop(). axis=1 - означает, что мы удаляем столбец,

а в случае axis=0 - означает, что мы удаляем строку.

Можно посмотреть результаты этих действий, вызвав метод head() и поле shape, которыми мы пользовались ранее, но сейчас нужно вызывать их от новой переменной training_points.

##   3. Обучить модель на обучающей выборке

![](https://raw.githubusercontent.com/MerkulovDaniil/TensorFlow_and_Keras_crash_course/master/ford_price.png)

**Шаг 3.1. Выбираем метод, который будем использовать**

Проще всего начать с простых методов.
Мы воспользуемся двумя методами для построения моделей и сравним их между собой:
* Линейная регрессия *linear regression*
* Лес решающих деревьев *random forest*

На выбор метода для построения модели влияет набор признаков, размер выборки, интуиция про то, какая связь между входными переменными и целевой. Но часто решение принимается исходя из того, какая модель сработала лучше.

Для корректной работы с методами построения моделей в python требуется загрузить специальную библиотеку
**sklearn**, программную библиотеку на языке python для машинного обучения и анализа данных.

Мы импортируем два модуля из этой библиотеки:
 * *linear_model* - тут находятся все линейные модели
 * *ensemble* - тут находятся модели на основе ансамблей

   Прежде чем начать делать ремонт, нужно подготовить инструменты для работы. Аналогично в нашем случае, прежде чем обучать модели, нужно создать их прототипы.  

Чтобы создать модель линейной регресии, пишем имя модуля 'linear_model', затем точку, затем название модели.


Чтобы создать модель случайного леса, пишем имя модуля ensemble, затем точку, затем название модели.


Обратите внимание, что для воспроизводимости результата на разных компьютерах необходимо для всех зафиксировать один параметр random_state. Например, можно установить для него значение 123.

У модели на основе случайного леса больше параметров. Рассмотрим наиболее важные:
* параметр *n_estimators* определяет, сколько деревьев в лесу,
* в параметре *max_depth* устанавливается, какая максимальная глубина у дерева,
* в параметре *min_samples_leaf* задается, какое максимальное число объектов может попасть в лист дерева.

Так как у модели на основе случайного решающего леса больше параметров, такая модель обычно обучается медленнее.
Кроме этого, на время обучения влияют значения параметров модели. Например, чем больше деревьев в лесе - тем дольше модель будет учиться.

**Шаг 3.2. Обучить модель**

Теперь, когда мы создали прототипы обеих моделей, можем их обучить с помощью обучающей выборки.

Для этого вызываем метод **fit()** у каждой модели и передаем ему на вход два аргумента:
таблицу входных признаков и столбец значений целевой переменной - (training_points, training_values)

Делаем тоже самое для модели решающего леса.

- Для двух разных моделей в sklearn методы для обучения модели не отличаются.
- Мы получили две обученные модели.
- Теперь необходимо провалидировать модели на новых тестовых данных.



## 4. Загрузить и предобработать данные для тестирования

**Шаг 4.1. Загрузим и проанализируем тестовые данные.**

Так как данные в формате xlsx (Excel), мы будем использовать специальную функцию
из библиотеки pandas для загрузки таких данных **read_excel**.

В функции передаем один атрибут: название файла, в котором находится таблица с данными.

Что важно посмотреть, после того, как мы загрузили данные?

проверить, что данные действительно загрузились
посмотреть на данные, чтобы удостовериться, что они правильные: колонки имеют те же названия, что и в таблице и т.д.

Посмотрим на размеры загруженной таблицы, так как мы видели только 5 строк

Для этого вызываем поле shape у нашей переменной test_data. Поле вызывается также как метод, но в конце скобки не ставятся (!), так как для поля не предусмотрена передача аргументов.

*Что означает первое и второе число?*
Таблица содержит 2104 строк (объектов) и 12 столбцов (признаков), включая выходной (целевой) признак. Также как в учебных данных до обучения.

Таблицу проверили, теперь можно приступать к обработке данных
Действуем аналогично тому, как делали с данными для обучения

Проверим, есть ли в данных пропуски. Для того чтобы это сделать, нужно обратиться вызвать у переменной *test_data* метод **info()**.

Цифры в каждой строчке обозначают количество заполненных (*non-null*) значений. 

Видно, что в данных содержатся пропуски, так как эти цифры не в каждой строчке совпадают с полным числом строк (2106).

Нам необходимо удалить пропуски. Для этого применяем метод dropna() к test_data

Кодируем нечисловые признаки таким же образом

**Шаг 4.2. Отделяем целевую переменную**

Нам нужно выделить в отдельную переменную *test_values* столбец из нашей таблицы, который соответствует определенной выше целевой переменной.

 Для этого мы у таблицы *test_data* в квадратных скобках указываем имя нужного столбца. 
 
 В нашем случае это имя записано в переменной *target_variable_name*.

 Отделим входные переменные от выходной (целевой), чтобы можно было построить модель предсказания целевой переменной по входным.

Для это нужно у переменной test_data вызвать метод drop().

Результат мы записываем в новую переменную test_points.

После выполнения запроса test_points будет содержать исходную таблицу без целевого столбца.

Обратите внимание, что в данном случае мы передаем два аргумента:

target_variable_name - название столбца цены, который мы ранее записали в эту переменную и теперь хотим удалить из training_data

axis=1 - означает, что мы удаляем столбец, а в случае axis=0 - означает, что мы удаляем строку


И проверяем результат записанный в test_points

Сравним наборы признаков в тестовой и обучающей выборке

# 5. Провалидировать модель на тестовой выборке

**Шаг 5.1. Сравнение моделей.**

Теперь мы готовы сравнить качество двух моделей! 😎

*1. Какая модель лучше?*

Получим прогнозы целевой переменной на тестовых данных для модели линейной регрессии м модели случайного леса.

Для этого вызовем у каждой модели метод **predict()**, в качестве аргумента передадим *test_points*.

Качество регрессионных моделей оценим двумя способами:
1. Сравним визуально прогнозы с настоящими ценами (тестовые с предсказанием)
2. Сравним метрики качества

Визуализируем прогноз линейной модели и настоящие значения из тестовой выборки

Визуализируем прогноз модели случайного леса и настоящие значения из тестовой выборки

Кажется, что лучше сработала модель случайного леса, так как точки на втором изображении расположены вдоль диагонали. На первом изображении видно, что для высоких настоящих цен модель линейной регрессии дает существенно заниженный результат.

Проверим, так ли это с помощью **метрик качества регрессионной модели**

Для корректного подсчета метрик качества модели в python требуется загрузить их из библиотеки **sklearn**.

Мы используем две метрики качества:
 * *mean_absolute_error* - средняя абсолютная ошибка $|y_i - \hat{y}_i|$
 * *mean_squared_error* - средняя квадратичная ошибка $(y_i - \hat{y}_i)^2$
 * *RMSE* - корень из *mean_squared_error*

   Подсчитаем ошибки для линейной модели.

Для этого вызовем методы mean_absolute_error() и mean_squared_error().

На вход им передается столбец настоящих значений test_values и столбец значений, предсказанных моделью линейной регрессии test_predictions_linear.

Подсчитаем ошибки для модели случайного леса.

Для этого вызовем методы mean_absolute_error() и mean_squared_error().

На вход им передается столбец настоящих значений test_values и столбец значений, предсказанных моделью линейной регрессии test_predictions_random_forest.

Теперь напечатаем полученные ошибки.

print("MAE: {0:7.2f}, RMSE: {1:7.2f}, R2: {2:7.2f} for linear model".format(
        mean_absolute_error(test_values, test_predictions_linear), 
        mean_squared_error(test_values, test_predictions_linear)**0.5, r2_score_linear_model))

print("MAE: {0:7.2f}, RMSE: {1:7.2f},  R2: {2:7.2f} for random forest model".format(
       mean_absolute_error(test_values, test_predictions_random_forest), 
       mean_squared_error(test_values, test_predictions_random_forest)**0.5, r2_score_random_forest_model))

Модель случайного леса работает лучше и визуально, и потому, что абсолютная и средне квадратичная ошибка меньше для линейной регресии.

Мы получили значения метрик ошибок наших моделей. Чтобы понять, насколько это нас утсраивает, важно взглянуть на исходный порядок цен на автомобили. Видно, что средняя цена имеет порядок 20 000 долларов, что означает, что полученная ошибка может удовлетворять предъявляемым требованиям к модели регрессии.

# Обзор результатов и  выводы

В данном дипломном проекте:

1. Загружены данные для обучения
   
2. Обработаны данные перед обучением модели
2.1 Определены наличие пропусков в данных
2.2 Избавились от пропусков в данных
2.3 Построены гистограммы для возможных значений признаков

3. Обучена модель на обучающей выборке

4. Загружены и предобработаны данные для тестирования

5. Провалидирована модель на тестовой выборке

   РАБОТА  ВЫПОЛНЕНА согласно Плана-алгоритма анализа данных (data mining)
   # **ВЫВОД
   # **Для нормального функционирования рынка продаж подержанных автомоблей
   # **осуществить оценку подержанного автомобиля - это достаточно трудная задача!

   # **Представленная дипломная работа продемонстрировала определение (по целевой переменной - цене) стоимости на подержанные автомобили (на примере марки) Ford
   # **путем предсказания справедливых цен с помощью построения регрессионных моделей и их анализа. 
   
Google Colab — это бесплатная среда для разработки и выполнения программного кода в облаке. 

Она предоставляет возможность писать и запускать код на языке Python, используя только браузер, без установки специальных программ на компьютер.

НАСТРОЙКА   ДОСТУПА

Войдите в аккаунт администратора (он не заканчивается на @gmail.com). 

Дополнительные сервисы Google. 

Нажмите Google Colab. 

Нажмите Статус сервиса.

ДОСТУП

После запуска команды Colab предложит ввести код авторизации. 

Открыв URL, вы должны предоставить сервису доступ к своему аккаунту. 

Тогда он выдаст код, который нужно будет вставить в поле, нажать ВВОД, и Google Colab подключится к хранилищу.

НАСТРОЙКА  с  GitHub

Save a copy in GitHub

Выбираем репозиторий Diplom_2024

ОСТАЛОСЬ  СДЕЛАТЬ и ПОДГРУЗИТЬ ПРЕЗЕНТАЦИЮ


СПАСИБО   GeekBrains за  великолепное обучение на протяжение 2-х лет  и настрой на дальнейшее постоянное  изучение и освоение нового в профессии инжененра  искусственного интеллекта!
